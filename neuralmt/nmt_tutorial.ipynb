{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "New version: Jetic Gu\n",
    "\n",
    "Original: https://github.com/A-Jacobson/minimal-nmt\n",
    "\n",
    "\n",
    "This is an updated python notebook that can help you with the Neural Machine Translation homework.\n",
    "The original notebook written by Jacobson uses legacy API of torchtext which is no longer supported. This new version is tested under Ubuntu 22.04 LTS, running PyTorch 2.1 and torchtext 0.16.0 which is the last active development version of torchtext.\n",
    "\n",
    "It should be noted that the current state-of-the-art is most likely using Transformer-based architecture, however knowledge in traditional RNN-based attention mechanism will still help you understand Multi-Headed Attention better.\n",
    "\n",
    "Also note that the attention implementation provided in this tutorial is not going to work as the solution to this homework. However, it should be useful for you to understand how attention can be implemented as part of the sequence to sequence model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "import tqdm\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "import torchtext\n",
    "from torchtext.datasets import multi30k, Multi30k\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convenience Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_text(sequence, lex):\n",
    "    \"\"\"\n",
    "    This function will allow you to use a vocab object to convert\n",
    "    a list of indices to original words. \n",
    "    \"\"\"\n",
    "    # To convert tokens to indices, use brackets directly as if a dict \n",
    "    pad_idx = lex['<pad>']\n",
    "    # to convert list of indices back to words, use get_itos method\n",
    "    return \" \".join([lex.get_itos()[int(i)] for i in sequence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Multi30k English/German parallel corpus for NMT, Build Lexicon\n",
    "TorchText takes care of tokenization, padding,  special character tokens and batching.\n",
    " \n",
    "Before we get started, run `python3 -m spacy download en_core_web_sm` as well as `python3 -m spacy download de_core_news_sm` to get the tokeniser.\n",
    "\n",
    "This is gonna take a while. Standby, go get a coffee or sth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# loading tokeniser\n",
    "spacy_de_tokeniser = spacy.load(\"de_core_news_sm\")\n",
    "spacy_en_tokeniser = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenise_de(text):\n",
    "    return [tok.text for tok in spacy_de_tokeniser(text.lower())]\n",
    "\n",
    "def tokenise_en(text):\n",
    "    return [tok.text for tok in spacy_en_tokeniser(text.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing tokenisation on training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29001/29001 [04:26<00:00, 108.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing tokenisation on validation set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1015/1015 [00:10<00:00, 96.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building source lexicon\n",
      "Moving on\n"
     ]
    }
   ],
   "source": [
    "# Define special symbols and indices\n",
    "pad_idx, sos_idx, eos_idx, unk_idx = 0, 1, 2, 3\n",
    "special_symbols = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "\n",
    "def load_dataset():\n",
    "    # We need to modify the URLs for the dataset since the links to the original dataset are broken\n",
    "    # Refer to https://github.com/pytorch/text/issues/1756#issuecomment-1163664163 for more info\n",
    "    multi30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\n",
    "    multi30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n",
    "    \n",
    "    # Load data, perform tokenisation\n",
    "    print(\"Performing tokenisation on training set\")\n",
    "    train_list = list(Multi30k(split='train', language_pair=(\"de\", \"en\")))\n",
    "    train_list = [(tokenise_de(f), tokenise_en(e)) for f, e in tqdm.tqdm(train_list)]\n",
    "    \n",
    "    print(\"Performing tokenisation on validation set\")\n",
    "    valid_list = list(Multi30k(split='valid', language_pair=(\"de\", \"en\")))\n",
    "    valid_list = [(tokenise_de(f), tokenise_en(e)) for f, e in tqdm.tqdm(valid_list)]\n",
    "\n",
    "    print(\"Building source lexicon\")\n",
    "    src_lex = torchtext.vocab.build_vocab_from_iterator([f for f, e in train_list],\n",
    "                                                        min_freq=1,\n",
    "                                                        specials=special_symbols,\n",
    "                                                        special_first=True)\n",
    "\n",
    "    tgt_lex = torchtext.vocab.build_vocab_from_iterator([e for f, e in train_list],\n",
    "                                                        min_freq=1,\n",
    "                                                        specials=special_symbols,\n",
    "                                                        special_first=True)\n",
    "    \n",
    "    # Set ``unk_idx`` as the default index. This index is returned when the token is not found.\n",
    "    # If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\n",
    "    src_lex.set_default_index(unk_idx)\n",
    "    tgt_lex.set_default_index(unk_idx)\n",
    "    print(\"Moving on\")\n",
    "\n",
    "    return train_list, valid_list, src_lex, tgt_lex\n",
    "\n",
    "train_list, valid_list, src_lex, tgt_lex = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inputs\n",
    "Model inputs are (seq_len, batch_size) Tensors of word indices.\n",
    "\n",
    "Creating batches for PyTorch 2.1 is different from legacy methods using bucketiterators. Instead, we'll write a batch sampler to create batches for every epoch, and a collate function to convert text for every batch to tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    1,     1,     1,     1,     1],\n",
       "         [    8,     5,     5,     5,  1548],\n",
       "         [ 1423,   493, 16106,    32,    48],\n",
       "         [    9,    10,    95,    83,     5],\n",
       "         [    5,     5,    42,    11,   224],\n",
       "         [ 1335,    49,     5,  2328,   236],\n",
       "         [   10,    74,   348,   995,     8],\n",
       "         [    5,    21,    11,    42, 15165],\n",
       "         [ 1299,     6,  3330,     5,    21],\n",
       "         [  469,   104,     4,   135,    15],\n",
       "         [   12,     7,     2,     4,   311],\n",
       "         [   14, 18346,     0,     2,   151],\n",
       "         [  136,  2686,     0,     0,   362],\n",
       "         [   12,   117,     0,     0,     4],\n",
       "         [    4,     4,     0,     0,     2],\n",
       "         [    2,     2,     0,     0,     0]]),\n",
       " tensor([[   1,    1,    1,    1,    1],\n",
       "         [   4,   21,    4,    4,  176],\n",
       "         [ 183,  387, 1588,   35,   10],\n",
       "         [ 781,   11,  705,   10,    4],\n",
       "         [  11,   55,  107,   79,  159],\n",
       "         [ 161,   17,   75,   60,  258],\n",
       "         [1371,   41,   76,    4, 5836],\n",
       "         [  11,  232,   74,   85,    8],\n",
       "         [1267,    4,   47,   13,    7],\n",
       "         [ 508, 6727,  476,  227,  156],\n",
       "         [   8,   42,  176, 1069,   12],\n",
       "         [   4, 1176,   17,  361,    7],\n",
       "         [ 149,   77,  717,   75,   77],\n",
       "         [   5,    5,    5,    5,    5],\n",
       "         [   2,    2,    2,    2,    2]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_batch(batch, src_lex, tgt_lex):\n",
    "    # This function is called to be executed upon loading a new batch into the trainer.\n",
    "    # It will transform the original text into tensor using the constructed lexicons\n",
    "    source, target = [], []\n",
    "    for f, e in batch:\n",
    "        # Appending sos and eos special tokens\n",
    "        source.append(torch.tensor([src_lex[f_tok] for f_tok in ['<sos>'] + f + ['<eos>']]))\n",
    "        target.append(torch.tensor([tgt_lex[e_tok] for e_tok in ['<sos>'] + e + ['<eos>']]))\n",
    "\n",
    "    # padding ensures source and target are of equal dimension across the batch, it also converts source and target to tensors-\n",
    "    source = pad_sequence(source, padding_value=pad_idx)\n",
    "    target = pad_sequence(target, padding_value=pad_idx)\n",
    "    return source, target\n",
    "\n",
    "\n",
    "def batch_sampler():\n",
    "    # sorts the whole dataset by target length, divide into batches, then shuffle the batches\n",
    "    # this function will be called every epoch\n",
    "    indices = [(i, len(e)) for i, (f, e) in enumerate(train_list)]\n",
    "    random.shuffle(indices)\n",
    "    pooled_indices = []\n",
    "    # create pool of indices with similar lengths \n",
    "    for i in range(0, len(indices), batch_size * 100):\n",
    "        pooled_indices.extend(sorted(indices[i:i + batch_size * 100], key=lambda x: x[1]))\n",
    "\n",
    "    pooled_indices = [x[0] for x in pooled_indices]\n",
    "    pools = []\n",
    "\n",
    "    # yield indices for current batch\n",
    "    for i in range(0, len(pooled_indices), batch_size):\n",
    "        pools.append(pooled_indices[i:i + batch_size])\n",
    "    random.shuffle(pools)\n",
    "    return pools\n",
    "    \n",
    "batch_size = 5\n",
    "train_dl = \\\n",
    "    DataLoader(train_list, batch_sampler=batch_sampler(),\n",
    "               collate_fn=lambda batch:collate_batch(batch, src_lex, tgt_lex))\n",
    "\n",
    "example_batch = next(iter(train_dl))\n",
    "example_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recover the original text by looking up each index in the vocabularies we build with the `load_data` function.\n",
    "\n",
    "The `<pad>` tokens here are for the trainer to ignore when calculating loss for the whole batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> eine sängerin , ein gitarrist und ein schlagzeuger treten auf einer bühne auf . <eos>\n",
      "<sos> a female singer and male guitarist and drummer perform on a stage . <eos>\n"
     ]
    }
   ],
   "source": [
    "example_batch_f, example_batch_e = example_batch\n",
    "print(sequence_to_text(example_batch_f[:, 0], src_lex))\n",
    "print(sequence_to_text(example_batch_e[:, 0], tgt_lex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture \n",
    "NMT uses an encoder-decoder architecture to effectively translate source sequences and target sequences that are of different lengths\n",
    "![img](assets/encoder-decoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "Encodes each word of the source sequence into a `hidden_dim` feature map. Sometimes called an `annotation`. Also returns the hidden state of the encoder bi-rnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, source_vocab_size, embed_dim, hidden_dim,\n",
    "                 n_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed = nn.Embedding(source_vocab_size, embed_dim, padding_idx=1)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, n_layers,\n",
    "                          dropout=dropout, bidirectional=True)\n",
    "\n",
    "    def forward(self, source, hidden=None):\n",
    "        embedded = self.embed(source)  # (batch_size, seq_len, embed_dim)\n",
    "        encoder_out, encoder_hidden = self.gru(\n",
    "            embedded, hidden)  # (seq_len, batch, hidden_dim*2)\n",
    "        # sum bidirectional outputs, the other option is to retain concat features\n",
    "        encoder_out = (encoder_out[:, :, :self.hidden_dim] +\n",
    "                       encoder_out[:, :, self.hidden_dim:])\n",
    "        return encoder_out, encoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(source_vocab_size=len(src_lex), embed_dim=embed_dim,\n",
    "                  hidden_dim=hidden_dim, n_layers=n_layers, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder output size:  torch.Size([16, 5, 512])\n",
      "encoder hidden size:  torch.Size([4, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "encoder_out, encoder_hidden = encoder(example_batch_f)\n",
    "print('encoder output size: ', encoder_out.size())  # source, batch_size, hidden_dim\n",
    "print('encoder hidden size: ', encoder_hidden.size()) # n_layers * num_directions, batch_size, hidden_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "Currently the `encoder_output` is a length 14 sequence and the target is a length 13 sequence. We need to compress the information in the `encoder_output` into a `context_vector` which should have all the information the decoder needs to predict the next step of its output. We will use `Luong Attention` to create this context vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    LuongAttention from Effective Approaches to Attention-based Neural Machine Translation\n",
    "    https://arxiv.org/pdf/1508.04025.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.W = nn.Linear(dim, dim, bias=False)\n",
    "\n",
    "    def score(self, decoder_hidden, encoder_out):\n",
    "        # linear transform encoder out (seq, batch, dim)\n",
    "        encoder_out = self.W(encoder_out)\n",
    "        # (batch, seq, dim) | (2, 15, 50)\n",
    "        encoder_out = encoder_out.permute(1, 0, 2)\n",
    "        # (2, 15, 50) @ (2, 50, 1)\n",
    "        return encoder_out @ decoder_hidden.permute(1, 2, 0)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_out):\n",
    "        energies = self.score(decoder_hidden, encoder_out)\n",
    "        mask = F.softmax(energies, dim=1)  # batch, seq, 1\n",
    "        context = encoder_out.permute(\n",
    "            1, 2, 0) @ mask  # (2, 50, 15) @ (2, 15, 1)\n",
    "        context = context.permute(2, 0, 1)  # (seq, batch, dim)\n",
    "        mask = mask.permute(2, 0, 1)  # (seq2, batch, seq1)\n",
    "        return context, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will normally be part of the decoder as it takes the previous decoder hidden state as input, but just to show the inputs and outputs I will use it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will initialize the Decoder rnn's hidden state with the last hidden state from the encoder. Because the encoder is bi-directional we have to reshape it's hidden state in order to select the layer we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 512])\n",
      "torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "attention = LuongAttention(dim=hidden_dim)\n",
    "context, mask = attention(encoder_hidden[-1:], encoder_out)\n",
    "print(context.size()) # (1, batch, attention_dim) contect_vector\n",
    "print(mask.size())  # the weights used to compute weighted sum over encoder out (1, batch, source_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, embed_dim, hidden_dim,\n",
    "                 n_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.embed = nn.Embedding(target_vocab_size, embed_dim, padding_idx=1)\n",
    "        self.attention = LuongAttention(hidden_dim)\n",
    "        self.gru = nn.GRU(embed_dim + hidden_dim, hidden_dim, n_layers,\n",
    "                          dropout=dropout)\n",
    "        self.out = nn.Linear(hidden_dim * 2, target_vocab_size)\n",
    "\n",
    "    def forward(self, output, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        decodes one output frame\n",
    "        \"\"\"\n",
    "        embedded = self.embed(output)  # (1, batch, embed_dim)\n",
    "        context, mask = self.attention(decoder_hidden[:-1], encoder_out)  # 1, 1, 50 (seq, batch, hidden_dim)\n",
    "        rnn_output, decoder_hidden = self.gru(torch.cat([embedded, context], dim=2),\n",
    "                                              decoder_hidden)\n",
    "        output = self.out(torch.cat([rnn_output, context], 2))\n",
    "        return output, decoder_hidden, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(target_vocab_size=len(tgt_lex), embed_dim=embed_dim,\n",
    "                  hidden_dim=hidden_dim, n_layers=n_layers, dropout=dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To translate one word from German to English, the decoder needs:\n",
    "1. `encoder_outputs`\n",
    "2. `decoder_hidden` initially, the last n_layers of encoder_hidden then it's own returned hidden state.\n",
    "3. `previous_output` feed a batch of start of string token (`sos_idx`, 1) at the first step.\n",
    "\n",
    "The attention mask that the decoder returns is not used in training but can be used to visualize where the decoder is \"looking\" in the input sequence in order to generate its current output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden = encoder_hidden[-decoder.n_layers:]\n",
    "start_token = example_batch_e[:1]\n",
    "start_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, decoder_hidden, mask = decoder(start_token, encoder_out, decoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output size:  torch.Size([1, 5, 9795])\n",
      "decoder hidden size  torch.Size([2, 5, 512])\n",
      "attention mask size torch.Size([1, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "print('output size: ', output.size())  # (1, batch, target_vocab) # predicted probability distribution over all possible target words\n",
    "print('decoder hidden size ', decoder_hidden.size())\n",
    "print('attention mask size', mask.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Helpers\n",
    "nmt models use teacher forcing during training and greedy decoding or beam search for inference. In order to accommodate these behaviors, I've made simple helper classes that get output from the decoder using each policy.\n",
    "\n",
    "The Teacher class sometimes feeds the previous target to the decoder rather than the model's previous prediction. this can help speed convergence but requires targets to be loaded to the helper at each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher:\n",
    "    def __init__(self, teacher_forcing_ratio=0.5):\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.targets = None\n",
    "        self.maxlen = 0\n",
    "        \n",
    "    def load_targets(self, targets):\n",
    "        self.targets = targets\n",
    "        self.maxlen = len(targets)\n",
    "\n",
    "    def generate(self, decoder, encoder_out, encoder_hidden):\n",
    "        outputs = []\n",
    "        masks = []\n",
    "        decoder_hidden = encoder_hidden[-decoder.n_layers:]  # take what we need from encoder\n",
    "        output = self.targets[0].unsqueeze(0)  # start token\n",
    "        for t in range(1, self.maxlen):\n",
    "            output, decoder_hidden, mask = decoder(output, encoder_out, decoder_hidden)\n",
    "            outputs.append(output)\n",
    "            masks.append(mask.data)\n",
    "            output = Variable(output.data.max(dim=2)[1])\n",
    "            # teacher forcing\n",
    "            is_teacher = random.random() < self.teacher_forcing_ratio\n",
    "            if is_teacher:\n",
    "                output = self.targets[t].unsqueeze(0)      \n",
    "        return torch.cat(outputs), torch.cat(masks).permute(1, 2, 0)  # batch, src, trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_helper = Teacher()\n",
    "decode_helper.load_targets(example_batch_e)\n",
    "outputs, masks = decode_helper.generate(decoder, encoder_out, encoder_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc loss\n",
    "reshape outputs and targets, ignore sos token at start of target batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.1786, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(outputs.view(-1, outputs.size(2)),\n",
    "                           example_batch_e[1:].view(-1), ignore_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The greedy decoder simply chooses the highest scoring word as output.\n",
    "We cam use the `set_maxlen` method to generate sequences the same length as our targets to easily check perplexity and bleu score during evaluation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Greedy:\n",
    "    def __init__(self, maxlen=20, sos_index=2):\n",
    "        self.maxlen = maxlen\n",
    "        self.sos_index = sos_index\n",
    "        \n",
    "    def set_maxlen(self, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        \n",
    "    def generate(self, decoder, encoder_out, encoder_hidden):\n",
    "        seq, batch, _ = encoder_out.size()\n",
    "        outputs = []\n",
    "        masks = []\n",
    "        decoder_hidden = encoder_hidden[-decoder.n_layers:]  # take what we need from encoder\n",
    "        output = Variable(torch.zeros(1, batch).long() + self.sos_index)  # start token\n",
    "        for t in range(self.maxlen):\n",
    "            output, decoder_hidden, mask = decoder(output, encoder_out, decoder_hidden)\n",
    "            outputs.append(output)\n",
    "            masks.append(mask.data)\n",
    "            output = Variable(output.data.max(dim=2)[1])\n",
    "        return torch.cat(outputs), torch.cat(masks).permute(1, 2, 0)  # batch, src, trg     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_helper = Greedy()\n",
    "decode_helper.set_maxlen(len(example_batch_e[1:]))\n",
    "outputs, masks = decode_helper.generate(decoder, encoder_out, encoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 5, 9795])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.1879, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(outputs.view(-1, outputs.size(2)),\n",
    "                           example_batch_e[1:].view(-1), ignore_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## seq2seq wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, decoding_helper):\n",
    "        encoder_out, encoder_hidden = self.encoder(source)\n",
    "        outputs, masks = decoding_helper.generate(self.decoder, encoder_out, encoder_hidden)\n",
    "        return outputs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq = Seq2Seq(encoder, decoder)\n",
    "decoding_helper = Teacher(teacher_forcing_ratio=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example iteration with wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_helper.load_targets(example_batch_e)\n",
    "outputs, masks = seq2seq(example_batch_f, decode_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([14, 5, 9795]), torch.Size([5, 16, 14]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.size(), masks.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.1940, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(outputs.view(-1, outputs.size(2)),\n",
    "                example_batch_e[1:].view(-1), ignore_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
